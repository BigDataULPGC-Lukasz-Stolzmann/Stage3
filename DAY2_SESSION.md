# Day 2 Session - Partitioning (2026-01-11)

## âœ… Co zrobiliÅ›my dzisiaj:

### 1. PrzeczytaliÅ›my caÅ‚Ä… dokumentacjÄ™:
- ARCHITECTURE.md - SWIM protocol, podstawy
- DAY1_SUMMARY.md - co zostaÅ‚o zaimplementowane Day 1
- INTEGRATION_PLAN.md - plan na kolejne dni
- Kod membership service (522 linie)

### 2. ZrozumieliÅ›my teoriÄ™ partycji:

**Problem bez partycji:**
```
hash(key) % num_nodes â†’ Node X

Node umiera â†’ num_nodes siÄ™ zmienia
â†’ ~92% danych musi siÄ™ przenieÅ›Ä‡! ğŸ’¥
```

**RozwiÄ…zanie z partycjami:**
```
Step 1: hash(key) % 256 â†’ Partition Y (STAÅA!)
Step 2: Partition Y â†’ Node X (zmienia siÄ™ przy failures)

Node umiera â†’ tylko jego partycje siÄ™ przenoszÄ…
â†’ ~25% danych (zamiast 92%)! âœ…
```

**Kluczowa insight:**
- Partycja = WIADRO na wiele kluczy
- TAK, wiele ksiÄ…Å¼ek moÅ¼e byÄ‡ w jednej partycji
- PrzykÅ‚ad: Partition 42 = [book_100, book_356, book_872, ...]

### 3. NapisaÅ‚eÅ› pierwszÄ… wersjÄ™ PartitionManager:

**Struktura:**
```rust
pub struct PartitionManager {
    num_partitions: u32,        // 256 (Hazelcast ma 271)
    replication_factor: usize,  // 1 (primary + backup)
    membership: Arc<MembershipService>,
}
```

**4 metody:**
1. `new()` - stworzenie âœ…
2. `get_partition(key)` - hash % 256 âœ…
3. `get_owners(partition)` - ring distribution âŒ BUGI!
4. `my_primary_partitions()` - filter âŒ BUGI!
5. `my_backup_partitions()` - filter âŒ BUGI!

---

## ğŸ› BÅ‚Ä™dy ktÃ³re popeÅ‚niÅ‚eÅ› (WAÅ»NE - popraw jutro!):

### BÅ‚Ä…d 1: `sort_by()` zwraca `()`, nie `Vec`!

```rust
// âŒ Å¹LE:
let sorted = alive_nodes.sort_by(|a, b| a.0.cmp(&b.0));
// sorted jest () (unit), nie Vec!

// âœ… DOBRZE:
let mut node_ids: Vec<NodeId> = alive_nodes
    .into_iter()
    .map(|node| node.id)  // Node â†’ NodeId
    .collect();

node_ids.sort_by(|a, b| a.0.cmp(&b.0));  // Sortuje in-place, zwraca ()
// Teraz node_ids jest posortowany!
```

### BÅ‚Ä…d 2: Cannot move out of `self.membership.local_node.id`

```rust
// âŒ Å¹LE:
let my_id = self.membership.local_node.id;  // PrÃ³ba move z &self

// âœ… DOBRZE:
let my_id = self.membership.local_node.id.clone();  // Clone!
```

### BÅ‚Ä…d 3: `owners[1]` moÅ¼e panic jeÅ›li tylko 1 node!

```rust
// âŒ Å¹LE:
owners.len() > 1 && &owners[1] == my_id  // Bounds check, ale & zbÄ™dne

// âœ… DOBRZE:
owners.len() > 1 && owners[1] == my_id  // SprawdÅº dÅ‚ugoÅ›Ä‡ PRZED [1]
```

---

## ğŸ“š Czego siÄ™ nauczyÅ‚eÅ› (Rust patterns):

### 1. Pattern matching w closure:
```rust
// Iterator zwraca &u32 (referencjÄ™):
(0..256)
    .filter(|&partition| {  // â† & destructure: &u32 â†’ u32
        // partition jest teraz u32, nie &u32
    })
```

**Dlaczego &partition?**
- `.filter()` wymaga `FnMut(&Item) -> bool` (przyjmuje referencjÄ™)
- `|&partition|` to pattern match: destructure `&u32` â†’ `u32`
- MoÅ¼esz teÅ¼: `|partition: &u32|` i uÅ¼ywaÄ‡ `*partition`

### 2. Closures POÅ»YCZAJÄ„, nie zjadajÄ…:
```rust
let my_id = /* ... */;

.filter(|&partition| {
    owners[0] == my_id  // â† BORROW (immutable reference)
})

// my_id nadal istnieje po filter()! âœ…
```

**Kiedy closure ZJADA (move)?**
- `move |x| { ... }` (keyword `move`)
- `tokio::spawn(async move { ... })` (wymagane dla async)
- Zwracanie closure z funkcji

### 3. `sort_by()` sortuje in-place:
```rust
let mut vec = vec![3, 1, 2];
vec.sort_by(|a, b| a.cmp(b));  // Zwraca (), ale vec teraz [1, 2, 3]
```

---

## ğŸŒ Teoria: Dlaczego partycje to industry standard?

| System | Liczba partycji | Notatki |
|--------|----------------|---------|
| **Hazelcast** | 271 (default) | Konfigurowalne, prod czÄ™sto 1000+ |
| **Redis Cluster** | 16,384 | Hash slots |
| **Cassandra** | 256 (vnodes) | Virtual nodes |
| **Riak** | 64-256 | Configurable |
| **Amazon Dynamo** | Dynamic | Paper opisuje virtual nodes |

**Dlaczego liczby pierwsze? (271 vs 256)**
- 271 = liczba pierwsza â†’ lepsze rozÅ‚oÅ¼enie dla dowolnej liczby nodÃ³w
- 256 = 2^8 â†’ szybszy modulo (bit shift)
- Dla 4 nodÃ³w: praktycznie bez rÃ³Å¼nicy

**Akademicko:**
> "UÅ¼yÅ‚em partition-based consistent hashing (Amazon Dynamo, Cassandra approach)
> z 256 partycjami dla minimalizacji data movement przy node failures."

ProwadzÄ…cy: ğŸ¤¯

---

## ğŸ¯ Co dalej (jutro - Day 2 cd.):

### 1. NAJPIERW: Popraw PartitionManager (3 bÅ‚Ä™dy wyÅ¼ej)

**Plik:** `src/storage/partitioner.rs`

**Struktura folderÃ³w (jeÅ›li nie zrobiÅ‚eÅ›):**
```bash
mkdir -p src/storage
touch src/storage/mod.rs
touch src/storage/partitioner.rs
```

**src/storage/mod.rs:**
```rust
pub mod partitioner;
```

**src/lib.rs (dodaj):**
```rust
pub mod membership;
pub mod storage;  // â† DODAJ tÄ™ liniÄ™
```

### 2. Test:
```bash
cargo build
cargo test
```

### 3. NastÄ™pnie: DistributedMap (HTTP + in-memory storage)

**BÄ™dzie zawieraÄ‡:**
- `put(key, value)` - zapisz (forward jeÅ›li nie jestem owner)
- `get(key)` - pobierz (fetch z ownera)
- `store_local()` - zapis w RAM (DashMap)
- HTTP communication miÄ™dzy nodami

**To bÄ™dzie fun! BÄ™dziesz wysyÅ‚aÄ‡ dane przez sieÄ‡!** ğŸš€

---

## ğŸ“‚ Struktura projektu (obecna):

```
distributed-cluster/
â”œâ”€â”€ Cargo.toml
â”œâ”€â”€ ARCHITECTURE.md
â”œâ”€â”€ DAY1_SUMMARY.md
â”œâ”€â”€ INTEGRATION_PLAN.md
â”œâ”€â”€ DAY2_SESSION.md          â† TEN PLIK (nowy!)
â””â”€â”€ src/
    â”œâ”€â”€ main.rs
    â”œâ”€â”€ lib.rs
    â”œâ”€â”€ membership/
    â”‚   â”œâ”€â”€ mod.rs
    â”‚   â”œâ”€â”€ types.rs
    â”‚   â””â”€â”€ service.rs       (522 linie, SWIM protocol âœ…)
    â””â”€â”€ storage/             â† DO ZROBIENIA JUTRO
        â”œâ”€â”€ mod.rs
        â””â”€â”€ partitioner.rs   (do poprawy!)
```

---

## ğŸ“ Poprawiony kod (uÅ¼yj jutro):

```rust
// src/storage/partitioner.rs

use crate::membership::{service::MembershipService, types::NodeId};
use std::collections::hash_map::DefaultHasher;
use std::hash::{Hash, Hasher};
use std::sync::Arc;

pub struct PartitionManager {
    num_partitions: u32,
    replication_factor: usize,
    membership: Arc<MembershipService>,
}

impl PartitionManager {
    pub fn new(membership: Arc<MembershipService>) -> Self {
        Self {
            num_partitions: 256,
            replication_factor: 1,
            membership,
        }
    }

    pub fn get_partition(&self, key: &str) -> u32 {
        let mut hasher = DefaultHasher::new();
        key.hash(&mut hasher);
        let hash = hasher.finish();
        (hash % self.num_partitions as u64) as u32
    }

    pub fn get_owners(&self, partition: u32) -> Vec<NodeId> {
        let alive_nodes = self.membership.get_alive_members();

        if alive_nodes.is_empty() {
            return vec![];
        }

        // POPRAWKA: Ekstraktuj NodeId i sortuj
        let mut node_ids: Vec<NodeId> = alive_nodes
            .into_iter()
            .map(|node| node.id)
            .collect();

        node_ids.sort_by(|a, b| a.0.cmp(&b.0));

        let primary_idx = (partition as usize) % node_ids.len();
        let backup_idx = (partition as usize + 1) % node_ids.len();

        vec![
            node_ids[primary_idx].clone(),
            node_ids[backup_idx].clone(),
        ]
    }

    pub fn my_primary_partitions(&self) -> Vec<u32> {
        let my_id = self.membership.local_node.id.clone();  // POPRAWKA: clone()

        (0..self.num_partitions)
            .filter(|&partition| {
                let owners = self.get_owners(partition);
                !owners.is_empty() && owners[0] == my_id
            })
            .collect()
    }

    pub fn my_backup_partitions(&self) -> Vec<u32> {
        let my_id = self.membership.local_node.id.clone();  // POPRAWKA: clone()

        (0..self.num_partitions)
            .filter(|&partition| {
                let owners = self.get_owners(partition);
                owners.len() > 1 && owners[1] == my_id  // POPRAWKA: bounds check
            })
            .collect()
    }
}
```

---

## ğŸ’¡ Jak wpuÅ›ciÄ‡ dane do klastra (zapytaÅ‚eÅ›):

### OdpowiedÅº krÃ³tka:
```bash
# PUT na DOWOLNY node:
curl -X POST http://192.168.1.10:6000/put \
  -d '{"key": "book_100", "value": {...}}'

# Node automatycznie:
# 1. Oblicza partition
# 2. Znajduje ownera
# 3. Forward jeÅ›li trzeba
# 4. Replicate do backup
```

**To zrobimy w DistributedMap jutro!**

---

## ğŸ“ Kluczowe insights z dzisiaj:

1. **Partycje = sprytne rozdrobnienie**
   - Zamiast 4 nody â†’ 256 partycji
   - Minimalizuje data movement (25% vs 92%)

2. **Partycja = wiadro na wiele kluczy**
   - Partition 42 = [book_100, book_356, book_872, ...]

3. **Industry standard**
   - Dynamo, Cassandra, Hazelcast, Redis - WSZYSCY uÅ¼ywajÄ…

4. **Rust patterns**
   - `sort_by()` in-place
   - Closures borrow by default
   - Pattern matching: `|&x|`

---

## âœ… Status projektu:

**Day 1 (DONE):**
- âœ… Membership service (SWIM gossip)
- âœ… Failure detection
- âœ… Auto-discovery
- âœ… Przetestowane na 2 komputerach

**Day 2 (W TRAKCIE):**
- â³ PartitionManager (napisany, ma bugi - popraw jutro!)
- â³ DistributedMap (TODO jutro)

**Day 3-7 (TODO):**
- Replication
- Task Executor
- HTTP API
- Integration z Stage2
- Testing na 4 maszynach

---

## ğŸš€ Jutro zaczynasz od:

1. Popraw 3 bÅ‚Ä™dy w PartitionManager (kod wyÅ¼ej)
2. `cargo build && cargo test`
3. Implementacja DistributedMap:
   - `put()` - HTTP forward
   - `get()` - HTTP fetch
   - DashMap storage w RAM

**Widzimy siÄ™ jutro!** ğŸŒ™

---

*Saved: 2026-01-11 Evening*
*Next: DistributedMap implementation*
